{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 600 first epochs were performed on the reduce dataset (360 000 images). Then to the complete dataset (560 000 images).\n",
    "\n",
    "With batch size of 640 and 360 000 images in the database, 100 epochs took about 3h (a bit less)\n",
    "With batch size of 700 and 560 000 images in the database, 100 epochs took about 4h40\n",
    "\n",
    "\n",
    "WITH METRICS: 200 epochs: Wall time: 11h 33min 22s\n",
    "\n",
    "Current script load the 'final' weigths (which is save if everything goes well)\n",
    "\n",
    "Weights are saved every 100 epochs. I should probably keep wieghts each 1000 epochs (depending on how many time it took to treat the whole data set) 139298/139937\n",
    "\n",
    "\n",
    "Look at the correlation matrix to see if there is a bias in the angle correct prediction. If yes, rebuild the dataset by applying angle of 90/180/270 to try to get something mor or less homogenous\n",
    "1 step: realign every centriole in a 0->90 orientation\n",
    "\n",
    "2ns step: perform reorientation. => rotation of 90/180/270 i can also flip the image\n",
    "\n",
    "Load the 500 epochs save if problem\n",
    "\n",
    "\n",
    "\n",
    "TO DO:\n",
    "\n",
    "To check: Note: in train definition i put the model in train mode (model.train() ) I do not change this statut after.\n",
    "\n",
    "Save the log of accuracy and loss\n",
    "\n",
    "Save the train_loader and validation_loader to don't have to rebuild them (especially because that lead to mixing each time train and validation set\n",
    "\n",
    "\n",
    "IMPORTANT: CHANGE PERFORMED ON gestion of classification/Regression problem (modification of n_classes/n_class call in data_agregator and VGG_schmidtea to modify the call of the number of class. MODIFICATION NOT TESTED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "\n",
    "import csv\n",
    "import pickle\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/cyril_b/projects/Planarians/tools/ToolBox.ipynb\n",
      "importing Jupyter notebook from /home/cyril_b/projects/Planarians/tools/CNN_Tools.ipynb\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 51)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/cyril_b/.virtualenvs/planarians/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3417\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-1-dfc2148fbe63>\"\u001b[0m, line \u001b[1;32m11\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from tools.CNN_Tools import VGG_Schmidtea\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m991\u001b[0m, in \u001b[1;35m_find_and_load\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m975\u001b[0m, in \u001b[1;35m_find_and_load_unlocked\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m655\u001b[0m, in \u001b[1;35m_load_unlocked\u001b[0m\n",
      "  File \u001b[1;32m\"<frozen importlib._bootstrap>\"\u001b[0m, line \u001b[1;32m618\u001b[0m, in \u001b[1;35m_load_backward_compatible\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/cyril_b/.virtualenvs/planarians/lib/python3.8/site-packages/import_ipynb.py\"\u001b[0;36m, line \u001b[0;32m61\u001b[0;36m, in \u001b[0;35mload_module\u001b[0;36m\u001b[0m\n\u001b[0;31m    exec(code, mod.__dict__)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m51\u001b[0m\n\u001b[0;31m    loss = criterion(output, angle)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "from tools.ToolBox import json_loader\n",
    "from tools.ToolBox import csv_saver\n",
    "\n",
    "from tools.CNN_Tools import VGG_Schmidtea\n",
    "from tools.CNN_Tools import train\n",
    "from tools.CNN_Tools import validate\n",
    "\n",
    "from tools.Dataset_Tools import centriole_dataset\n",
    "from tools.Dataset_Tools import dataset_creator\n",
    "from tools.Dataset_Tools import dataset_loader\n",
    "from tools.Dataset_Tools import dataset_aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the model that you want to FineTune\n",
    "WEIGHT_PATH = './weight/VGG_schmidtea_weight_classification.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = 'regression'\n",
    "#problem = 'classification'\n",
    "n_class = 72\n",
    "\n",
    "Loss = 1\n",
    "\n",
    "if problem == 'classification':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # n class expected in the new model\n",
    "    nClass = 360\n",
    "    \n",
    "elif problem == 'regression_1':\n",
    "    # Angle in degree\n",
    "    def criterion(output, target):\n",
    "        a = output - target\n",
    "        a = (a + 180) % 360 - 180\n",
    "        loss = a ** 2\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    nClass = 1\n",
    "   \n",
    "elif problem == 'regression_2':\n",
    "    def criterion(output, target):\n",
    "        loss = 2*(1-torch.cos(output - target))\n",
    "        \n",
    "        return loss\n",
    "    nClass = 1\n",
    "        \n",
    "\n",
    "elif problem == 'regression_3' :\n",
    "    def criterion(output, target):\n",
    "       \"\"\" LOSS FUNCTION FOR POSITION ANGLE BASED ON MINIMUM ARC SEPARATION \"\"\"\n",
    "       loss = torch.mean(torch.stack([ (output-target)**2,\n",
    "                                      (1-torch.abs(output-target))**2] ).min(dim=0)[0])\n",
    "       return loss\n",
    "\n",
    "    nClass = 1\n",
    "    \n",
    "elif problem == 'regression_4':\n",
    "    \n",
    "    nClass = 2\n",
    "\n",
    "## Model that you want to modify\n",
    "n_class = 72\n",
    "model = VGG_Schmidtea(n_classes = n_class)#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extracting: we update only the last layer so we set the requires.grad to false\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset = True\n",
    "\n",
    "# Creation/Loading of the Dataset\n",
    "if load_dataset == False:   \n",
    "    train_loader, validation_loader = dataset_creator(path_json = './data_json/',\n",
    "                                                     batch_size = 700,\n",
    "                                                     n_class = n_class,\n",
    "                                                     save_dataset = True)  \n",
    "elif problem == 'classification':\n",
    "    train_loader, validation_loader = dataset_loader(path = './data/',\n",
    "                                                     train_set = 'data_train_pclassification_n360_b700_normalized.pth',\n",
    "                                                     val_set = 'validation_data_pclassification_n360_b700_.pth')\n",
    "    \n",
    "else: \n",
    "    train_loader, validation_loader = dataset_loader(path = './data/',\n",
    "                                                 train_set = 'train_data_pregression_b700_unNormalized.pth',\n",
    "                                                 val_set = 'validation_data_pregression_b700_unNormalized.pth')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.6.0  Device: cuda\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './weight/VGG_schmidtea_weigth_classification.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-526a7ae8d500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Weight loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mload_weight\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWEIGHT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mset_parameter_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/planarians/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/planarians/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/planarians/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './weight/VGG_schmidtea_weigth_classification.pth'"
     ]
    }
   ],
   "source": [
    "load_weight = True\n",
    "\n",
    "# For feature extraction: only the last layer is updated\n",
    "feature_extract = True\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():                                  \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
    "\n",
    "\n",
    "\n",
    "# Weight loading\n",
    "if load_weight == True:\n",
    "    model.load_state_dict(torch.load(WEIGHT_PATH))\n",
    "    set_parameter_requires_grad(model, feature_extract)\n",
    "    model.classifier[8] = nn.Linear(4096, nClass)\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "params_to_update = model.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "    \n",
    "\n",
    "\n",
    "# Oprimizer loading\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.)\n",
    "\n",
    "print('\\n')\n",
    "print(\"\\t\", model.classifier[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Problem: {problem}\")\n",
    "print(f\"Number of Class: {nClass}\")\n",
    "print(f\"Criterion : {criterion}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/559300 (0%)]\tLoss: 6.085548\n",
      "\n",
      "Training set: Average loss: 5.9605, Accuracy: 1852/559300 (0%)\n",
      "outside loop\n",
      "Validation set: Average loss: 5.9331, Accuracy: 511/139937 (0%)\n",
      "\n",
      "CPU times: user 3min 7s, sys: 37.2 ms, total: 3min 7s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the number of epochs to train the model\n",
    "epochs = 1\n",
    "performed_epochs = 0\n",
    "\n",
    "\n",
    "# Let's Train!!\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Instantiate variable to log loss and accuracy\n",
    "    acct, losst, accv, lossv,cmt, cmv  = [], [], [], [], [], []\n",
    "    \n",
    "    train(model, losst, acct, cmt, train_loader, device, problem, criterion, optimizer, epoch, nClass)    \n",
    "    validate(model, lossv, accv, cmv, validation_loader, device, problem, criterion, nClass)\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        torch.save(model.state_dict(),('./weight/VGG_schmidtea_weigth_epoch_classification_n360_' + str(epoch+performed_epochs) + '.pth'))\n",
    "    \n",
    "    \"\"\"\n",
    "    csv_saver(accv , './metrics/accv.csv')\n",
    "    csv_saver(lossv, './metrics/lossv.csv')\n",
    "    csv_saver(acct , './metrics/acct.csv')\n",
    "    csv_saver(losst, './metrics/losst.csv')\n",
    "    csv_saver(cmt  , './metrics/cmt.csv')\n",
    "    csv_saver(cmv  , './metrics/cmv.csv')\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "#torch.save(model.state_dict(),('VGG_schmidtea_weigth_epoch_final.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 tensor([[[ 123.,  119.,  137.,  ...,   94.,   85.,   89.],\n",
      "         [ 130.,  164.,  203.,  ...,   84.,   94.,   94.],\n",
      "         [ 172.,  212.,  247.,  ...,   94.,   86.,   98.],\n",
      "         ...,\n",
      "         [  91.,  120.,  121.,  ..., 2031., 1725., 1353.],\n",
      "         [  84.,   99.,  114.,  ..., 1871., 1683., 1354.],\n",
      "         [  96.,  115.,  102.,  ..., 1631., 1507., 1197.]]],\n",
      "       dtype=torch.float64)\n",
      "tensor([337,  50, 339, 178, 240, 342,  14, 335,  10,  39,  24, 249, 142,  72,\n",
      "         47, 301,  31, 169, 348,  22])\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(validation_loader):\n",
    "    if idx == 131:\n",
    "        print(len(batch['image']), batch['image'][362])\n",
    "        print((batch['angle'][350:370]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD CONFUSION MATRIX IN REGISTERED METRICS\n",
    "\n",
    "In CNN_TOOLS , add conf_matrix as arguments and add the arguments here too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e8550e5ae284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Copy data to GPU if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mangle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'angle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "confusion_matrix = torch.zeros(nClass, nClass)\n",
    "\n",
    "#model.load_state_dict(torch.load('./weight/VGG_schmidtea_weigth_epoch_1200.pth'))\n",
    "\n",
    "for batch_idx, batch in enumerate(validation_loader):\n",
    "    # Copy data to GPU if needed\n",
    "    img = batch['image'].float().to(device)\n",
    "    angle = batch['angle'].long().to(device)\n",
    "\n",
    "    with torch.no_grad():   \n",
    "        output = model(img)  \n",
    "\n",
    "    # get the index of the max log-probability\n",
    "    pred_angle = output.max(1)[1]\n",
    "\n",
    "    for a, p in zip(angle.view(-1), pred_angle.view(-1)):\n",
    "        confusion_matrix[a.long(), p.long()] +=1\n",
    "            \n",
    "# Data in %\n",
    "col_sum = confusion_matrix.numpy().sum(axis=1)\n",
    "col_sum = np.reshape(col_sum, [n_class, -1])\n",
    "\n",
    "confusion_matrix = confusion_matrix / col_sum\n",
    "    \n",
    "df_cm = pd.DataFrame(confusion_matrix.numpy())\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "sn.heatmap(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(72):\n",
    "    print(\"Number of {} : {}\".format(i, angle_db.count(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss_function(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(My_loss_function, self).__init__()\n",
    "        \n",
    "    def forward(self, output, labels):\n",
    "        labels = labels.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train = centriole_dataset(img_db = x_train, angle_db = y_train)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(dataset_train)):\n",
    "    sample = dataset_train[i+10]\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('angle {}°'.format(int(sample['angle'])))\n",
    "    ax.axis('off')\n",
    "    show_centriole(np.array(sample['image'], dtype = 'uint8'))\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = centriole_dataset(img_db = x_test, angle_db = y_test)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(dataset_test)):\n",
    "    sample = dataset_test[i+10]\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('angle {}°'.format(int(sample['angle'])))\n",
    "    ax.axis('off')\n",
    "    show_centriole(np.array(sample['image'], dtype = 'uint8'))\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
