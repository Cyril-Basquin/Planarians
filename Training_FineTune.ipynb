{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 600 first epochs were performed on the reduce dataset (360 000 images). Then to the complete dataset (560 000 images).\n",
    "\n",
    "With batch size of 640 and 360 000 images in the database, 100 epochs took about 3h (a bit less)\n",
    "With batch size of 700 and 560 000 images in the database, 100 epochs took about 4h40\n",
    "\n",
    "\n",
    "WITH METRICS: 200 epochs: Wall time: 11h 33min 22s\n",
    "\n",
    "Current script load the 'final' weigths (which is save if everything goes well)\n",
    "\n",
    "Weights are saved every 100 epochs. I should probably keep wieghts each 1000 epochs (depending on how many time it took to treat the whole data set) 139298/139937\n",
    "\n",
    "\n",
    "Look at the correlation matrix to see if there is a bias in the angle correct prediction. If yes, rebuild the dataset by applying angle of 90/180/270 to try to get something mor or less homogenous\n",
    "1 step: realign every centriole in a 0->90 orientation\n",
    "\n",
    "2ns step: perform reorientation. => rotation of 90/180/270 i can also flip the image\n",
    "\n",
    "Load the 500 epochs save if problem\n",
    "\n",
    "\n",
    "\n",
    "TO DO:\n",
    "\n",
    "To check: Note: in train definition i put the model in train mode (model.train() ) I do not change this statut after.\n",
    "\n",
    "Save the log of accuracy and loss\n",
    "\n",
    "Save the train_loader and validation_loader to don't have to rebuild them (especially because that lead to mixing each time train and validation set\n",
    "\n",
    "\n",
    "IMPORTANT: CHANGE PERFORMED ON gestion of classification/Regression problem (modification of n_classes/n_class call in data_agregator and VGG_schmidtea to modify the call of the number of class. MODIFICATION NOT TESTED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "\n",
    "import csv\n",
    "import pickle\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/cyril_b/projects/Planarians/tools/ToolBox.ipynb\n",
      "importing Jupyter notebook from /home/cyril_b/projects/Planarians/tools/CNN_Tools.ipynb\n",
      "importing Jupyter notebook from /home/cyril_b/projects/Planarians/tools/Dataset_Tools.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "from tools.ToolBox import json_loader\n",
    "from tools.ToolBox import csv_saver\n",
    "\n",
    "from tools.CNN_Tools import VGG_Schmidtea\n",
    "from tools.CNN_Tools import train\n",
    "from tools.CNN_Tools import validate\n",
    "\n",
    "from tools.Dataset_Tools import centriole_dataset\n",
    "from tools.Dataset_Tools import dataset_creator\n",
    "from tools.Dataset_Tools import dataset_loader\n",
    "from tools.Dataset_Tools import dataset_aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the model that you want to FineTune\n",
    "WEIGHT_PATH = './weight/VGG_schmidtea_weight_classification.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = 'regression_4'\n",
    "#problem = 'classification'\n",
    "n_class = 1\n",
    "\n",
    "Loss = 1\n",
    "\n",
    "if problem == 'classification':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # n class expected in the new model\n",
    "    nClass = 360\n",
    "    \n",
    "elif problem == 'regression_1':\n",
    "    # Angle in degree\n",
    "    def criterion(output, target):\n",
    "        a = output - target\n",
    "        a = (a + 180) % 360 - 180\n",
    "        loss = a ** 2\n",
    "        \n",
    "        return loss.sum()\n",
    "    \n",
    "    nClass = 1\n",
    "   \n",
    "elif problem == 'regression_2':\n",
    "    def criterion(output, target):\n",
    "        loss = 2*(1-torch.cos(output - target))\n",
    "        \n",
    "        return loss.sum()\n",
    "    nClass = 1\n",
    "        \n",
    "\n",
    "elif problem == 'regression_3' :\n",
    "    def criterion(output, target):\n",
    "       \"\"\" LOSS FUNCTION FOR POSITION ANGLE BASED ON MINIMUM ARC SEPARATION \"\"\"\n",
    "       loss = torch.mean(torch.stack([ (output-target)**2,\n",
    "                                      (1-torch.abs(output-target))**2] ).min(dim=0)[0])\n",
    "       return loss.sum()\n",
    "\n",
    "    nClass = 1\n",
    "    \n",
    "elif problem == 'regression_4':\n",
    "    criterion = nn.MSELoss()\n",
    "    nClass = 1\n",
    "\n",
    "## Model that you want to modify\n",
    "model = VGG_Schmidtea(n_classes = n_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extracting: we update only the last layer so we set the requires.grad to false\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset = True\n",
    "\n",
    "# Creation/Loading of the Dataset\n",
    "if load_dataset == False:   \n",
    "    train_loader, validation_loader = dataset_creator(path_json = './data_json/',\n",
    "                                                     batch_size = 700,\n",
    "                                                     n_class = n_class,\n",
    "                                                     save_dataset = True)  \n",
    "elif problem == 'classification':\n",
    "    train_loader, validation_loader = dataset_loader(path = './data/',\n",
    "                                                     train_set = 'data_train_pclassification_n360_b700_normalized.pth',\n",
    "                                                     val_set = 'validation_data_pclassification_n360_b700_.pth')\n",
    "    \n",
    "else: \n",
    "    train_loader, validation_loader = dataset_loader(path = './data/',\n",
    "                                                 train_set = 'train_data_pregression_b700_unNormalized.pth',\n",
    "                                                 val_set = 'validation_data_pregression_b700_unNormalized.pth')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.6.0  Device: cpu\n",
      "Params to learn:\n",
      "\t conv_1.0.weight\n",
      "\t conv_1.0.bias\n",
      "\t conv_1.1.weight\n",
      "\t conv_1.1.bias\n",
      "\t conv_1.2.weight\n",
      "\t conv_1.2.bias\n",
      "\t conv_2.0.weight\n",
      "\t conv_2.0.bias\n",
      "\t conv_2.1.weight\n",
      "\t conv_2.1.bias\n",
      "\t conv_2.2.weight\n",
      "\t conv_2.2.bias\n",
      "\t conv_3.0.weight\n",
      "\t conv_3.0.bias\n",
      "\t conv_3.1.weight\n",
      "\t conv_3.1.bias\n",
      "\t conv_3.2.weight\n",
      "\t conv_3.2.bias\n",
      "\t conv_3.3.weight\n",
      "\t conv_3.3.bias\n",
      "\t conv_4.0.weight\n",
      "\t conv_4.0.bias\n",
      "\t conv_4.1.weight\n",
      "\t conv_4.1.bias\n",
      "\t conv_4.2.weight\n",
      "\t conv_4.2.bias\n",
      "\t conv_4.3.weight\n",
      "\t conv_4.3.bias\n",
      "\t conv_5.0.weight\n",
      "\t conv_5.0.bias\n",
      "\t conv_5.1.weight\n",
      "\t conv_5.1.bias\n",
      "\t conv_5.2.weight\n",
      "\t conv_5.2.bias\n",
      "\t conv_5.3.weight\n",
      "\t conv_5.3.bias\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "\t classifier.3.weight\n",
      "\t classifier.3.bias\n",
      "\t classifier.5.weight\n",
      "\t classifier.5.bias\n",
      "\t classifier.7.weight\n",
      "\t classifier.7.bias\n",
      "\t classifier.8.weight\n",
      "\t classifier.8.bias\n",
      "\t Linear(in_features=4096, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "load_weight = False\n",
    "\n",
    "# For feature extraction: only the last layer is updated\n",
    "feature_extract = True\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():                                  \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
    "\n",
    "\n",
    "\n",
    "# Weight loading\n",
    "if load_weight == True:\n",
    "    model.load_state_dict(torch.load(WEIGHT_PATH))\n",
    "    set_parameter_requires_grad(model, feature_extract)\n",
    "    model.classifier[8] = nn.Linear(4096, nClass)\n",
    "    model.to(device)\n",
    "\n",
    "else: \n",
    "    model.to(device)\n",
    "    \n",
    "params_to_update = model.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "    \n",
    "\n",
    "\n",
    "# Oprimizer loading\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.)\n",
    "\n",
    "print(\"\\t\", model.classifier[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem: regression_4\n",
      "Number of Class: 1\n",
      "Criterion : MSELoss()\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Problem: {problem}\")\n",
    "print(f\"Number of Class: {nClass}\")\n",
    "print(f\"Criterion : {criterion}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/559746 (0%)]\tLoss: 47816.187500\n",
      "Validation loss: 0.0\n",
      "Train Epoch: 2 [0/559746 (0%)]\tLoss: nan\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the number of epochs to train the model\n",
    "epochs = 3\n",
    "performed_epochs = 0\n",
    "\n",
    "\n",
    "# Let's Train!!\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Instantiate variable to log loss and accuracy\n",
    "    acct, losst, accv, lossv,cmt, cmv  = [], [], [], [], [], []\n",
    "    list_target = []\n",
    "    \n",
    "    train(model, losst, acct, cmt, train_loader, device, problem, criterion, optimizer, epoch, nClass)    \n",
    "    validate(model, lossv, accv, cmv, validation_loader, device, problem, criterion, nClass, list_target)\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        torch.save(model.state_dict(),('./weight/VGG_schmidtea_weigth_epoch_classification_n360_' + str(epoch+performed_epochs) + '_TOTO.pth'))\n",
    "    \n",
    "    \"\"\"\n",
    "    csv_saver(accv , './metrics/accv.csv')\n",
    "    csv_saver(lossv, './metrics/lossv.csv')\n",
    "    csv_saver(acct , './metrics/acct.csv')\n",
    "    csv_saver(losst, './metrics/losst.csv')\n",
    "    csv_saver(cmt  , './metrics/cmt.csv')\n",
    "    csv_saver(cmv  , './metrics/cmv.csv')\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "#torch.save(model.state_dict(),('VGG_schmidtea_weigth_epoch_final.pth')) comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "toto = 0\n",
    "for i in list_target:\n",
    "    for j in i[0]:\n",
    "        x_list.append(torch.flatten(j).cpu().detach().numpy())\n",
    "    for j in i[1]:\n",
    "        y_list.append(j.cpu().detach().numpy())\n",
    "        \n",
    "            \n",
    "print(len(list_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nulber of list in i\", len(list_target))\n",
    "print(\"nulber of list in j\", len(list_target[0]))\n",
    "print(\"nulber of list in j\", len(list_target[0][0]))\n",
    "\n",
    "\n",
    "len(x_list)\n",
    "len(y_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int(y_list[3]))\n",
    "print(int(x_list[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x_list, y_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(validation_loader):\n",
    "    if idx == 131:\n",
    "        print(len(batch['image']), batch['image'][362])\n",
    "        print((batch['angle'][350:370]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD CONFUSION MATRIX IN REGISTERED METRICS\n",
    "\n",
    "In CNN_TOOLS , add conf_matrix as arguments and add the arguments here too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix = torch.zeros(nClass, nClass)\n",
    "\n",
    "#model.load_state_dict(torch.load('./weight/VGG_schmidtea_weigth_epoch_1200.pth'))\n",
    "\n",
    "for batch_idx, batch in enumerate(validation_loader):\n",
    "    # Copy data to GPU if needed\n",
    "    img = batch['image'].float().to(device)\n",
    "    angle = batch['angle'].long().to(device)\n",
    "\n",
    "    with torch.no_grad():   \n",
    "        output = model(img)  \n",
    "\n",
    "    # get the index of the max log-probability\n",
    "    pred_angle = output.max(1)[1]\n",
    "\n",
    "    for a, p in zip(angle.view(-1), pred_angle.view(-1)):\n",
    "        confusion_matrix[a.long(), p.long()] +=1\n",
    "            \n",
    "# Data in %\n",
    "col_sum = confusion_matrix.numpy().sum(axis=1)\n",
    "col_sum = np.reshape(col_sum, [n_class, -1])\n",
    "\n",
    "confusion_matrix = confusion_matrix / col_sum\n",
    "    \n",
    "df_cm = pd.DataFrame(confusion_matrix.numpy())\n",
    "\n",
    "plt.figure(figsize = (12,8))\n",
    "sn.heatmap(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(72):\n",
    "    print(\"Number of {} : {}\".format(i, angle_db.count(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss_function(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(My_loss_function, self).__init__()\n",
    "        \n",
    "    def forward(self, output, labels):\n",
    "        labels = labels.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train = centriole_dataset(img_db = x_train, angle_db = y_train)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(dataset_train)):\n",
    "    sample = dataset_train[i+10]\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('angle {}°'.format(int(sample['angle'])))\n",
    "    ax.axis('off')\n",
    "    show_centriole(np.array(sample['image'], dtype = 'uint8'))\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = centriole_dataset(img_db = x_test, angle_db = y_test)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(dataset_test)):\n",
    "    sample = dataset_test[i+10]\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('angle {}°'.format(int(sample['angle'])))\n",
    "    ax.axis('off')\n",
    "    show_centriole(np.array(sample['image'], dtype = 'uint8'))\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, device='cuda')\n",
    "device_id = x.device.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
